{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fed79ee0e6bd26d348d02b8c7b59886c3afe22c2"
   },
   "source": [
    "# Import and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-17T02:10:31.176Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Data process\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestRegressor as RFF\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# Statis\n",
    "from scipy.stats import kurtosis, skew\n",
    "from sklearn.metrics import roc_auc_score,mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "# Utilities\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "\n",
    "\n",
    "# Plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (18,12)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e32583cc487663dd61799cb1960f872eff20bd2c"
   },
   "source": [
    "# Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb_cv(X,y,params=None,n_round=50,n_fold=5,categorical_feature=[]):\n",
    "    # credits : https://www.kaggle.com/delayedkarma/let-s-add-some-new-features-lb-0-674\n",
    "    folds = KFold(n_splits=n_fold, shuffle=False, random_state=15)\n",
    "    oof = np.zeros(len(X))\n",
    "    clfs = []\n",
    "\n",
    "    score = [0 for _ in range(folds.n_splits)]\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print(\"Fold no.{}\".format(fold_+1))\n",
    "        trn_data = lgb.Dataset(X.iloc[trn_idx],\n",
    "                               label=y.iloc[trn_idx],\n",
    "                               categorical_feature = categorical_feature\n",
    "                              )\n",
    "        val_data = lgb.Dataset(X.iloc[val_idx],\n",
    "                               label=y.iloc[val_idx],\n",
    "                               categorical_feature = categorical_feature\n",
    "                              )\n",
    "        \n",
    "        clf = lgb.train(params,\n",
    "                        trn_data,\n",
    "                        n_round,\n",
    "                        valid_sets = [trn_data, val_data],\n",
    "                        verbose_eval=50,\n",
    "                        early_stopping_rounds = 50)\n",
    "        clfs.append(clf)\n",
    "\n",
    "        oof[val_idx] = clf.predict(X.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "\n",
    "        score[fold_] = roc_auc_score(y.iloc[val_idx], oof[val_idx])\n",
    "\n",
    "    \n",
    "    print(\"CV score: {:<8.5f}\".format(roc_auc_score(y, oof)*100))\n",
    "\n",
    "    return clfs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb(X,y,rate=.8,n_round=50,params=None):\n",
    "    if params is None:\n",
    "        params = {\n",
    "              \"objective\": \"binary\",\n",
    "#               \"boosting_type\": \"gbdt\",\n",
    "              \"learning_rate\": 0.02,\n",
    "              \"max_depth\": 8,\n",
    "              \"num_leaves\": 67,\n",
    "              \"n_estimators\": n_round,\n",
    "              \"bagging_fraction\": 0.4,\n",
    "              \"feature_fraction\": 0.5,\n",
    "              \"bagging_freq\": 5,\n",
    "              \"bagging_seed\": 2018,\n",
    "              \"min_child_samples\": 80,\n",
    "              \"min_child_weight\": 100,\n",
    "              \"min_split_gain\": 0.1,\n",
    "              \"reg_alpha\": 0.005,\n",
    "              \"reg_lambda\": 0.1,\n",
    "              \"subsample_for_bin\": 25000,\n",
    "              \"min_data_per_group\": 100,\n",
    "              \"max_cat_to_onehot\": 4,\n",
    "              \"cat_l2\": 25,\n",
    "              \"cat_smooth\": 2,\n",
    "              \"max_cat_threshold\": 32,\n",
    "              \"random_state\": 1,\n",
    "              \"silent\": True,\n",
    "              \"metric\": \"auc\"\n",
    "        }\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=1-rate, random_state=42)\n",
    "\n",
    "    # LightGBM Regressor estimator\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        verbose=20, eval_metric='auc',\n",
    "        early_stopping_rounds=n_round\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_valid, num_iteration=model.best_iteration_)\n",
    "\n",
    "    score = roc_auc_score(y_valid, y_pred)\n",
    "    print(f'Final score {score*100:.2f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lgb_cv(clfs,X,categorical_feature=[]):\n",
    "    preds = np.zeros((len(X),len(clfs)))\n",
    "    t = time.time()\n",
    "    for i,clf in enumerate(clfs):\n",
    "        preds[:,i] = clf.predict(X)\n",
    "        print(f\"clf {i} predicted in {time.time()-t}\")\n",
    "        t = time.time()\n",
    "    y_ = np.mean(preds,axis=1)\n",
    "    return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d48f9ea4532b68da11401a92d6dc020f33c791a7"
   },
   "outputs": [],
   "source": [
    "def run_xgb_cv(X,y,rate=.8,params=None,n_round=50,nfold=5):\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'eta': 0.05,\n",
    "            'max_depth': 5,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'objective': 'reg:linear',\n",
    "            'eval_metric': 'auc',\n",
    "            'silent': 1\n",
    "        }\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=1-rate, random_state=42)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid,y_valid)\n",
    "    \n",
    "    model = xgb.cv(params, dtrain, num_boost_round=n_round, early_stopping_rounds=20,\n",
    "                        verbose_eval=50, show_stdv=False,nfold=nfold,metrics='auc')\n",
    "    y_pred = model.predict(X_valid, num_iteration=model.best_iteration_)\n",
    "\n",
    "    score = roc_auc_score(y_valid, y_pred)\n",
    "    \n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24e84945849d90581707fd71c587ead4ba709583"
   },
   "outputs": [],
   "source": [
    "def run_xgb(X,y,data=None,rate=.9,params=None,n_round=50):\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'eta': 0.05,\n",
    "            'max_depth': 5,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'objective': 'reg:linear',\n",
    "            'eval_metric': 'auc',\n",
    "            'silent': 1\n",
    "        }\n",
    "    \n",
    "    if data is None:\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=1-rate, random_state=42)\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train, y_train)\n",
    "        dval = xgb.DMatrix(X_valid,y_valid)\n",
    "    else:\n",
    "        dtrain,dval = data\n",
    "\n",
    "    model = xgb.train(params, dtrain, num_boost_round=n_round, early_stopping_rounds=50,\n",
    "                        evals= [(dtrain, 'train'), (dval, 'valid')],\n",
    "                        verbose_eval=20)\n",
    "    \n",
    "    return model\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
